#include "example/common/tiny_shakespeare_dataset.h"

#include <cstddef>
#include <cstdint>
#include <cstdlib>
#include <filesystem>
#include <fstream>
#include <functional>
#include <iostream>
#include <memory>
#include <numeric>
#include <string>
#include <unordered_map>
#include <utility>
#include <variant>
#include <vector>

#include "glog/logging.h"

#include "infini_train/include/tensor.h"

namespace {
using DataType = infini_train::DataType;
using TinyShakespeareType = TinyShakespeareDataset::TinyShakespeareType;
using TinyShakespeareFile = TinyShakespeareDataset::TinyShakespeareFile;

const std::unordered_map<int, TinyShakespeareType> kTypeMap = {
    {20240520, TinyShakespeareType::kUINT16}, // GPT-2
    {20240801, TinyShakespeareType::kUINT32}, // LLaMA 3
};

const std::unordered_map<TinyShakespeareType, size_t> kTypeToSize = {
    {TinyShakespeareType::kUINT16, 2},
    {TinyShakespeareType::kUINT32, 4},
};

const std::unordered_map<TinyShakespeareType, DataType> kTypeToDataType = {
    {TinyShakespeareType::kUINT16, DataType::kUINT16},
    {TinyShakespeareType::kUINT32, DataType::kINT32},
};

std::vector<uint8_t> ReadSeveralBytesFromIfstream(size_t num_bytes, std::ifstream *ifs) {
    std::vector<uint8_t> result(num_bytes);
    ifs->read(reinterpret_cast<char *>(result.data()), num_bytes);
    return result;
}

template <typename T> T BytesToType(const std::vector<uint8_t> &bytes, size_t offset) {
    static_assert(std::is_trivially_copyable<T>::value, "T must be trivially copyable.");
    T value;
    std::memcpy(&value, &bytes[offset], sizeof(T));
    return value;
}

TinyShakespeareFile ReadTinyShakespeareFile(const std::string &path, size_t sequence_length) {
    /* =================================== ä½œä¸š ===================================
       TODOï¼šå®ç°äºŒè¿›åˆ¶æ•°æ®é›†æ–‡ä»¶è§£æ
       æ–‡ä»¶æ ¼å¼è¯´æ˜ï¼š
    ----------------------------------------------------------------------------------
    | HEADER (1024 bytes)                     | DATA (tokens)                        |
    | magic(4B) | version(4B) | num_toks(4B) | reserved(1012B) | tokenæ•°æ®           |
    ----------------------------------------------------------------------------------
       =================================== ä½œä¸š =================================== */

    std::ifstream fin(path, std::ios::binary);
    CHECK(fin) << "Failed to open file" << path;

    // magic
    // the magic number is used for identify dataset
    // and to specify the kTypeMap
    // how do I know????? (I cheated by looking the InfiniTensor Official Repo)
    // From https://github.com/InfiniTensor/InfiniTrain/blob/master/example/common/tiny_shakespeare_dataset.cc
    TinyShakespeareFile text_file;
    const auto header = ReadSeveralBytesFromIfstream(1024, &fin);
    const int32_t magic = BytesToType<int32_t>(header, 0); // sec param is offset from beginning
    const int32_t version = BytesToType<int32_t>(header, 4);
    const int32_t num_tokens = BytesToType<int32_t>(header, 8);
    text_file.type = kTypeMap.at(magic);

    // There are tokens of size `num_tokens`
    // The tokens are divided into sequence, whose size is fixed as `sequence_length`
    // Therefore, there are `num_sequences` sequences
    const int num_sequences = num_tokens / sequence_length;
    // vector assign: replace the content with
    // 1. count & values
    // 2. from first to last
    // 3. init list
    // Each sequence is mapped to a tensor,
    //      therefore, the dim is of size `num_sequences` and each dim is of same size `sequence_length`
    // The signature of `dims` is `std::vector<int64_t>`, so it is nec to map before use
    // x: There is no padding processing!
    // Actually there is, the last sequence will be dropped if not complete
    text_file.dims.assign(num_sequences, static_cast<int64_t>(sequence_length));

    const int data_size_in_bytes
        = std::accumulate(text_file.dims.begin(),
                        text_file.dims.end(),
                        kTypeToSize.at(text_file.type), // change the init value here
                         std::multiplies<int>{}); // use {} instead
    // Init tensor, type is kINIT64? why?
    text_file.tensor = infini_train::Tensor(text_file.dims, DataType::kINT64);
    // let data_ptr_ pointing to the data 
    int64_t *dst = static_cast<int64_t *>(text_file.tensor.DataPtr());

    // // å®‰å…¨çš„ unionï¼Œè¿™é‡Œè¦ä¹ˆå­˜ std::vector<uint16_t>, è¦ä¹ˆå­˜ std::vector<int32_t>
    // std::variant<std::vector<uint16_t>, std::vector<int32_t>> buffer;
    // if (text_file.type == TinyShakespeareType::kUINT16) {
    //     CHECK_LE(sequence_length, 1024); // GPT-2: max_seq_length = 1024
    //     buffer = std::vector<uint16_t>(num_sequences * sequence_length);
    // } else if (text_file.type == TinyShakespeareType::kUINT32) {
    //     CHECK_LE(sequence_length, 8192); // LLaMA-3: max_seq_length = 8192
    //     buffer = std::vector<int32_t>(num_sequences * sequence_length);
    // }

    // // ç”¨ visit æ¥è®¿é—®å¹¶ castã€‚
    // std::visit(
    //     // æ•è·å¤–éƒ¨å˜é‡ï¼Œé€šè¿‡ å¼•ç”¨
    //     // [=] å°±æ˜¯æ•è· å€¼
    //     [&](auto &vec) {
    //         fin.read(reinterpret_cast<char *>(vec.data()), data_size_in_bytes);
    //         for (size_t i = 0; i < vec.size(); ++i) { dst[i] = static_cast<int64_t>(vec[i]); }
    //     },
    //     buffer);

    // æ„Ÿè§‰ä¸Šé¢å¾ˆå¤æ‚ã€‚ã€‚ã€‚ æ˜æ˜éƒ½ if äº†ï¼Œ ä¸å¦‚ç›´æ¥ cast
    if (text_file.type == TinyShakespeareType::kUINT16) {
        CHECK_LE(sequence_length, 1024); // GPT-2: max_seq_length = 1024
        std::vector<uint16_t> vec(num_sequences * sequence_length);
        fin.read(reinterpret_cast<char *>(vec.data()), data_size_in_bytes);
        for (size_t i = 0; i < vec.size(); ++i) { dst[i] = static_cast<int64_t>(vec[i]); }
    } else if (text_file.type == TinyShakespeareType::kUINT32) {
        CHECK_LE(sequence_length, 8192); // LLaMA-3: max_seq_length = 8192
        std::vector<int32_t> vec(num_sequences * sequence_length);
        fin.read(reinterpret_cast<char *>(vec.data()), data_size_in_bytes);
        for (size_t i = 0; i < vec.size(); ++i) { dst[i] = static_cast<int64_t>(vec[i]); }
    }
    return text_file;
}
} // namespace

// TinyShakespeareDataset::TinyShakespeareDataset(const std::string &filepath, size_t sequence_length) {
//     // =================================== ä½œä¸š ===================================
//     // TODOï¼šåˆå§‹åŒ–æ•°æ®é›†å®ä¾‹
//     // HINT: è°ƒç”¨ReadTinyShakespeareFileåŠ è½½æ•°æ®æ–‡ä»¶
//     // =================================== ä½œä¸š ===================================
//     text_file_ = ReadTinyShakespeareFile(filepath, sequence_length);
//     // x: num_samples_ æ˜¯ num_sequence
//     // è¦å‡ä¸€ï¼Œå› ä¸º
//     // å¦‚æœä½ æœ‰ N ä¸ª tokensï¼ŒæŠŠå®ƒåˆ‡æˆè‹¥å¹²é•¿åº¦ä¸º sequence_length çš„åºåˆ—ã€‚
//     // é‚£ä¹ˆï¼š
//     // ç¬¬ä¸€ä¸ª batch ç”¨ tokens [0..sequence_length-1] ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ [1..sequence_length]ã€‚
//     // ç¬¬äºŒä¸ª batch ç”¨ tokens [1..sequence_length] ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ [2..sequence_length+1]ã€‚
//     // â€¦ä»¥æ­¤ç±»æ¨ã€‚
//     // ğŸ‘‰ ä½ ä¼šå‘ç°ï¼šæœ€åä¸€ä¸ª token æ²¡æ³•é¢„æµ‹â€œä¸‹ä¸€ä¸ªâ€ï¼Œå› ä¸ºæ•°æ®å·²ç»æ²¡æœ‰äº†ã€‚
//     // æ‰€ä»¥ï¼š
//     // å¦‚æœåŸæ¥æœ‰ num_sequences ä¸ªç‰‡æ®µï¼Œ
//     // å®é™…ä¸Šèƒ½ç”¨æ¥è®­ç»ƒçš„åªæœ‰ num_sequences - 1ã€‚
//     num_samples_ = text_file_.dims[0]; 
//     sequence_size_in_bytes_ = sequence_length * kTypeToSize.at(text_file_.type);
// }

// ç”¨ infinitrain çš„ å†™æ³•ï¼Œç›´æ¥æˆå‘˜åˆå§‹åŒ–
TinyShakespeareDataset::TinyShakespeareDataset(const std::string &filepath, size_t sequence_length)
    : text_file_(ReadTinyShakespeareFile(filepath, sequence_length)), sequence_length_(sequence_length),
      sequence_size_in_bytes_(sequence_length * sizeof(int64_t)), num_samples_(text_file_.dims[0] - 1) {
    // =================================== ä½œä¸š ===================================
    // TODOï¼šåˆå§‹åŒ–æ•°æ®é›†å®ä¾‹
    // HINT: è°ƒç”¨ReadTinyShakespeareFileåŠ è½½æ•°æ®æ–‡ä»¶
    // =================================== ä½œä¸š ===================================
    // ç”¨ infinitrain çš„ å†™æ³•ï¼Œç›´æ¥æˆå‘˜åˆå§‹åŒ–
    CHECK_EQ(text_file_.dims[1], sequence_length_);
    CHECK_EQ(static_cast<int>(text_file_.tensor.Dtype()), static_cast<int>(DataType::kINT64));
}

std::pair<std::shared_ptr<infini_train::Tensor>, std::shared_ptr<infini_train::Tensor>>
TinyShakespeareDataset::operator[](size_t idx) const {
    CHECK_LT(idx, text_file_.dims[0] - 1);
    std::vector<int64_t> dims = std::vector<int64_t>(text_file_.dims.begin() + 1, text_file_.dims.end());
    // x: (seq_len), y: (seq_len) -> stack -> (bs, seq_len) (bs, seq_len)
    return {std::make_shared<infini_train::Tensor>(text_file_.tensor, idx * sequence_size_in_bytes_, dims),
            std::make_shared<infini_train::Tensor>(text_file_.tensor, idx * sequence_size_in_bytes_ + sizeof(int64_t),
                                                   dims)};
}

size_t TinyShakespeareDataset::Size() const { return num_samples_; }
